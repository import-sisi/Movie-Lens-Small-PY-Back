# -*- coding: utf-8 -*-
"""Movie Lens Small Latest Datasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nq_tinzJM7dsYWCM9y_okHm6CgWQkzP9
"""

import pandas as pd
from zipfile import ZipFile
from io import BytesIO
from urllib.request import urlopen

# Load the datasets
# movies_df = pd.read_csv('movies.csv')
# ratings_df = pd.read_csv('ratings.csv')
# tags_df = pd.read_csv('tags.csv')
# links_df = pd.read_csv('links.csv')
movielens_url = "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
response = urlopen(movielens_url)
with ZipFile(BytesIO(response.read())) as z:
    z.extractall()

movies_df = pd.read_csv("ml-latest-small/movies.csv")
ratings_df = pd.read_csv("ml-latest-small/ratings.csv")
tags_df = pd.read_csv("ml-latest-small/tags.csv")
links_df = pd.read_csv("ml-latest-small/links.csv")


# Display the first few rows of each dataframe to understand their structure
movies_df = movies_df.drop_duplicates()
ratings_df = ratings_df.drop_duplicates()
tags_df = tags_df.drop_duplicates()
links_df = links_df.drop_duplicates()

ratings_df['datetime'] = pd.to_datetime(ratings_df['timestamp'], unit='s')
ratings_df = ratings_df.drop(['timestamp'], axis=1)
tags_df['datetime'] = pd.to_datetime(tags_df['timestamp'], unit='s')
tags_df = tags_df.drop(['timestamp'], axis=1)

# I want to keep only the row with the earliest datetime for each combination of userId and movieId, and remove any rows with later datetimes for the same userId and movieId combination.
df = pd.DataFrame(tags_df)
# Convert 'datetime' column to datetime objects
df['datetime'] = pd.to_datetime(df['datetime'])
# Group by 'userId' and 'movieId', and find the index of the earliest datetime for each group
idx = df.groupby(['userId', 'movieId'])['datetime'].idxmin()
# Filter the DataFrame to keep only rows with the earliest datetime for each userId-movieId combination
tags_df = df.loc[idx]


# dfs = pd.DataFrame(movies_df)
# movies_with_genres = dfs.dropna(subset=['movieId'])
# movies_with_genres
movies_df, ratings_df, tags_df, links_df
# has_duplicates_A = ratings_df.duplicated('movieId').any()
# print(f"Column 'A' has duplicates: {has_duplicates_A}")

from sklearn.preprocessing import MultiLabelBinarizer

# Split the genres into lists
movies_df['genres_list'] = movies_df['genres'].str.split('|')

# Initialize the MultiLabelBinarizer
mlb = MultiLabelBinarizer()

# Fit and transform the genres to one-hot encoding
genres_encoded = mlb.fit_transform(movies_df['genres_list'])
genres_encoded_df = pd.DataFrame(genres_encoded, columns=mlb.classes_, index=movies_df['movieId'])

# Display the first few rows of the one-hot encoded genres dataframe

# # Filter ratings for high ratings (4.0 and above)
# high_ratings_df = ratings_df[ratings_df['rating'] >= 4.0]

# # Merge high ratings with the one-hot encoded genres dataframe
# user_genre_preferences = high_ratings_df.merge(genres_encoded_df, left_on='movieId', right_index=True)

# # Aggregate genres for each user to create user profiles
# user_profiles = user_genre_preferences.groupby('userId')[mlb.classes_].sum()

# # Display the first few rows of the user profiles dataframe
# user_profiles

merged_df = pd.merge(ratings_df, tags_df, on=['userId', 'movieId'], how='left', suffixes=('_rating', '_tag'))
# print(merged_df)
merged_df['earliest_datetime'] = merged_df[['datetime_rating', 'datetime_tag']].min(axis=1)
earliest_rows = merged_df.groupby(['userId', 'movieId'])['earliest_datetime'].idxmin()
filtered_df = merged_df.loc[earliest_rows]
has_NaT_values = filtered_df['rating'].isna().any()

if has_NaT_values:
    print("yes")
else:
    print("no")

merged_df = pd.merge(filtered_df, genres_encoded_df, on='movieId', how='left')
# merged_df.to_csv('merged_df.csv', index=False)

# Reload the original dataset to work on a fresh copy
df = merged_df

# Define the genre columns (excluding the 'tag', 'datetime_tag', and non-genre columns)
genre_columns = df.columns[df.columns.str.contains('Action|Adventure|Comedy|Crime|Drama|Family|Fantasy|Film-Noir|Horror|IMAX|Musical|Mystery|Romance|Sci-Fi|Thriller|War|Western')]

# Group the dataset by the combination of genre columns and apply a function to fill NaN values in 'tag'
def fill_tag_within_group(group):
    if group['tag'].isnull().all():  # If all values are NaN, return the group unchanged
        return group
    else:
        most_common_tag = group['tag'].mode().iloc[0] if not group['tag'].mode().empty else "Unknown"
        group['tag'] = group['tag'].fillna(most_common_tag)
        return group

df_filled = df.groupby(list(genre_columns)).apply(fill_tag_within_group).reset_index(drop=True)

# Count the number of NaN values in the "tag" column after operation
nan_count_after_filling = df_filled['tag'].isna().sum()

# Save the modified dataset to a new CSV file
filled_file_path = 'filled_merged_df.csv'
df_filled.to_csv(filled_file_path, index=False)

filled_merged_df = pd.read_csv('filled_merged_df.csv')
df_dropped_columns = filled_merged_df.drop(columns=['(no genres listed)', 'datetime_tag', 'datetime_rating'])

import numpy as np

# def df_dropped_columns(days_since_earliest, max_days, half_life=365):
#     """
#     Apply a simple exponential decay based on the number of days since the earliest rating.
#     """
#     normalized_days = days_since_earliest / max_days
#     decay_factor = np.exp(-np.log(2) * normalized_days / (half_life / max_days))
#     return decay_factor

# # Assuming 'ratings_df' and 'days_since_earliest' are properly defined and calculated
# max_days = df_dropped_columns['earliest_datetime'].max()
# ratings_df['decay_factor'] = ratings_df['days_since_earliest'].apply(lambda x: apply_time_decay(x, max_days))


# Ensure 'earliest_datetime' is in datetime format
df_dropped_columns['earliest_datetime'] = pd.to_datetime(df_dropped_columns['earliest_datetime'])

# Calculate the max datetime to use for calculating days_since_earliest
max_datetime = df_dropped_columns['earliest_datetime'].max()

# Calculate days_since_earliest for each row
df_dropped_columns['days_since_earliest'] = (max_datetime - df_dropped_columns['earliest_datetime']).dt.days
df_dropped_columns
# Apply the corrected time decay function
def apply_time_decay(days_since_earliest, half_life=365):
    """
    Apply a simple exponential decay based on the number of days since the earliest rating.
    """
    # The exponential decay formula, corrected to not divide half-life by max_days again
    decay_factor = np.exp(-np.log(2) * days_since_earliest / half_life)
    return decay_factor

# Apply the time decay to each rating
df_dropped_columns['decay_factor'] = df_dropped_columns['days_since_earliest'].apply(apply_time_decay)

# Display the updated dataframe with the decay factor for verification
# df_dropped_columns[['userId', 'movieId', 'rating', 'earliest_datetime', 'days_since_earliest', 'decay_factor']].head()
# df_dropped_columns
# df_dropped_columns.to_csv('df_dropped_columns.csv', index=False)
# print(df_dropped_columns.columns)

import pandas as pd
import torch
from torch import nn, optim
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# Load the dataset (assuming 'df_dropped_columns' is your DataFrame)
df = df_dropped_columns

# Encoders for userId and movieId
user_encoder = LabelEncoder()
movie_encoder = LabelEncoder()
df['userId'] = user_encoder.fit_transform(df['userId'])
df['movieId'] = movie_encoder.fit_transform(df['movieId'])

# Aggregate genre indicators into a one-hot encoded vector
genre_columns = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary',
                 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance',
                 'Sci-Fi', 'Thriller', 'War', 'Western']

# Split the dataset into training and validation sets
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# Function to prepare DataLoader
def prepare_data_loader(df, user_encoder, movie_encoder, genre_columns, batch_size=32):
    user_ids = torch.tensor(user_encoder.transform(df['userId'].values), dtype=torch.long)
    movie_ids = torch.tensor(movie_encoder.transform(df['movieId'].values), dtype=torch.long)
    genres = torch.tensor(df[genre_columns].values, dtype=torch.float)
    ratings = torch.tensor(df['rating'].values, dtype=torch.float).unsqueeze(1)
    dataset = TensorDataset(user_ids, movie_ids, genres, ratings)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_loader = prepare_data_loader(train_df, user_encoder, movie_encoder, genre_columns)
val_loader = prepare_data_loader(val_df, user_encoder, movie_encoder, genre_columns)

# Model definition
class UpdatedNCFModel(nn.Module):
    def __init__(self, num_users, num_items, num_genres, embedding_dim=32):
        super(UpdatedNCFModel, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        self.genre_linear = nn.Linear(num_genres, embedding_dim)
        self.fc_layers = nn.Sequential(
            nn.Linear(embedding_dim * 2 + embedding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, user_ids, item_ids, genres):
        user_embedded = self.user_embedding(user_ids)
        item_embedded = self.item_embedding(item_ids)
        genre_embedded = self.genre_linear(genres)
        combined_features = torch.cat([user_embedded, item_embedded, genre_embedded], dim=-1)
        return self.fc_layers(combined_features)

# Instantiate the model
num_users = df['userId'].nunique()
num_items = df['movieId'].nunique()
num_genres = len(genre_columns)
model = UpdatedNCFModel(num_users, num_items, num_genres)

# Train and evaluate the model
def train_and_evaluate_model(model, train_loader, val_loader, epochs=5, lr=0.001):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        train_losses = []
        for user_ids, movie_ids, genres, ratings in train_loader:
            optimizer.zero_grad()
            outputs = model(user_ids, movie_ids, genres)
            loss = criterion(outputs, ratings)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

        train_mse = np.mean(train_losses)

        model.eval()
        val_losses = []
        with torch.no_grad():
            for user_ids, movie_ids, genres, ratings in val_loader:
                outputs = model(user_ids, movie_ids, genres)
                loss = criterion(outputs, ratings)
                val_losses.append(loss.item())

        val_mse = np.mean(val_losses)

        print(f'Epoch {epoch+1}: Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}')

train_and_evaluate_model(model, train_loader, val_loader, epochs=5)

# Save the trained model and encoders for deployment
torch.save({
    'model_state_dict': model.state_dict(),
    'user_encoder': user_encoder,
    'movie_encoder': movie_encoder,
}, 'model_and_encoders.pth')

# Load the model and encoders for inference (if needed)
checkpoint = torch.load('model_and_encoders.pth')
model.load_state_dict(checkpoint['model_state_dict'])
user_encoder = checkpoint['user_encoder']
movie_encoder = checkpoint['movie_encoder']
##########################################################

# Assuming the necessary imports and UpdatedNCFModel class definition are already in place

# Load the model and encoders for inference
checkpoint = torch.load('model_and_encoders.pth')
model.load_state_dict(checkpoint['model_state_dict'])
user_encoder = checkpoint['user_encoder']
movie_encoder = checkpoint['movie_encoder']

# Prepare the recommendation function
def recommend_movies(model, user_id, excluded_movie_ids):
    user_id_encoded = user_encoder.transform([user_id])[0]
    excluded_movie_ids_encoded = movie_encoder.transform(excluded_movie_ids)

    all_movie_ids = set(range(len(movie_encoder.classes_))) - set(excluded_movie_ids_encoded)
    filtered_movie_ids = torch.tensor(list(all_movie_ids), dtype=torch.long)

    # Assume all genres and other inputs are needed; you might adjust this part based on your actual model input
    # For simplicity, this example does not include dynamic genre or other inputs for the excluded movies
    # You would need to adjust the logic here to match your model's input requirements

    # Dummy inputs for genres and other features, adjust based on your model's needs
    genres_dummy = torch.zeros((len(filtered_movie_ids), len(genre_columns)), dtype=torch.float)
    days_since_earliest_dummy = torch.zeros(len(filtered_movie_ids), dtype=torch.float)
    decay_factors_dummy = torch.zeros(len(filtered_movie_ids), dtype=torch.float)

    # Prepare inputs for the model
    filtered_user_ids = torch.full_like(filtered_movie_ids, fill_value=user_id_encoded, dtype=torch.long)

    # Make predictions
    with torch.no_grad():
        model.eval()
        predictions = model(filtered_user_ids, filtered_movie_ids, genres_dummy).squeeze()

    # Select the best movie to recommend
    best_movie_idx = predictions.argmax().item()
    best_movie_id = movie_encoder.inverse_transform([filtered_movie_ids[best_movie_idx]])[0]

    return best_movie_id

# Example usage
user_id = 1  # Example user ID, ensure this is a valid ID from your dataset
excluded_movie_ids = [10, 20, 30]  # Example excluded movie IDs, ensure these are valid IDs from your dataset
recommended_movie_id = recommend_movies(model, user_id, excluded_movie_ids)

print(f"Recommended Movie ID: {recommended_movie_id}")
##############################################################























import pandas as pd
import torch
from torch import nn, optim
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = df_dropped_columns

# Encoders for userId and movieId
user_encoder = LabelEncoder()
movie_encoder = LabelEncoder()

df['userId'] = user_encoder.fit_transform(df['userId'])
df['movieId'] = movie_encoder.fit_transform(df['movieId'])

# Aggregate genre indicators into a one-hot encoded vector
genre_columns = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary',
                 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance',
                 'Sci-Fi', 'Thriller', 'War', 'Western']
genres = df[genre_columns].values

# Prepare tensors
user_ids = torch.tensor(df['userId'].values, dtype=torch.long)
movie_ids = torch.tensor(df['movieId'].values, dtype=torch.long)
genres = torch.tensor(genres, dtype=torch.float)
days_since_earliest = torch.tensor(df['days_since_earliest'].values, dtype=torch.float)
decay_factors = torch.tensor(df['decay_factor'].values, dtype=torch.float)

# Model definition
class UpdatedNCFModel(nn.Module):
    def __init__(self, num_users, num_items, num_genres, embedding_dim=32):
        super(UpdatedNCFModel, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        self.genre_linear = nn.Linear(num_genres, embedding_dim)
        self.fc_layers = nn.Sequential(
            nn.Linear(embedding_dim * 2 + embedding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, user_ids, item_ids, genres):
        user_embedded = self.user_embedding(user_ids)
        item_embedded = self.item_embedding(item_ids)
        genre_embedded = self.genre_linear(genres)
        combined_features = torch.cat([user_embedded, item_embedded, genre_embedded], dim=-1)
        return self.fc_layers(combined_features)

# Model instantiation
num_users = df['userId'].nunique()
num_items = df['movieId'].nunique()
num_genres = len(genre_columns)
model = UpdatedNCFModel(num_users, num_items, num_genres)

# Recommendation function
def recommend_movies(model, user_id_encoded, excluded_movie_ids, genres, days_since_earliest, decay_factors, user_encoder, movie_encoder):
    # Assume user_id_encoded is already encoded
    all_movie_ids = set(range(num_items)) - set(excluded_movie_ids)
    filtered_movie_ids = torch.tensor(list(all_movie_ids), dtype=torch.long)

    # Prepare inputs for the model
    filtered_user_ids = torch.full_like(filtered_movie_ids, fill_value=user_id_encoded)
    filtered_genres = genres[filtered_movie_ids]
    filtered_days_since_earliest = days_since_earliest[filtered_movie_ids]
    filtered_decay_factors = decay_factors[filtered_movie_ids]

    # Make predictions
    predictions = model(filtered_user_ids, filtered_movie_ids, filtered_genres).squeeze()

    # Select the best movie to recommend
    best_movie_idx = predictions.argmax().item()
    best_movie_id = movie_encoder.inverse_transform([filtered_movie_ids[best_movie_idx]])[0]

    return best_movie_id

# Example usage
user_id = 1  # Example user ID
user_id_encoded = user_encoder.transform([user_id])[0]
excluded_movie_ids = [10, 20, 90]  # Example excluded movie IDs, encoded
recommended_movie_id = recommend_movies(model, user_id_encoded, excluded_movie_ids, genres, days_since_earliest, decay_factors, user_encoder, movie_encoder)
print(f"Recommended Movie ID: {recommended_movie_id}")

# Model saving
torch.save({
    'model_state_dict': model.state_dict(),
    'user_encoder': user_encoder,
    'movie_encoder': movie_encoder,
}, 'model_and_encoders.pth')

# To load the model and encoders for inference
checkpoint = torch.load('model_and_encoders.pth')
model.load_state_dict(checkpoint['model_state_dict'])
user_encoder = checkpoint['user_encoder']
movie_encoder = checkpoint['movie_encoder']

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# Split the dataset into training and validation sets
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# Prepare the data loaders
def prepare_data_loader(df, batch_size=32):
    user_ids = torch.tensor(user_encoder.transform(df['userId'].values), dtype=torch.long)
    movie_ids = torch.tensor(movie_encoder.transform(df['movieId'].values), dtype=torch.long)
    genres = torch.tensor(df[genre_columns].values, dtype=torch.float)
    ratings = torch.tensor(df['rating'].values, dtype=torch.float).unsqueeze(1) # Make sure ratings are a column vector
    dataset = TensorDataset(user_ids, movie_ids, genres, ratings)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_loader = prepare_data_loader(train_df)
val_loader = prepare_data_loader(val_df)

# Define the training loop
def train_model(model, train_loader, val_loader, epochs=5, lr=0.001):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        for user_ids, movie_ids, genres, ratings in train_loader:
            optimizer.zero_grad()
            outputs = model(user_ids, movie_ids, genres)
            loss = criterion(outputs, ratings)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * user_ids.size(0)
        train_loss /= len(train_loader.dataset)

        # Validation loss
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for user_ids, movie_ids, genres, ratings in val_loader:
                outputs = model(user_ids, movie_ids, genres)
                loss = criterion(outputs, ratings)
                val_loss += loss.item() * user_ids.size(0)
        val_loss /= len(val_loader.dataset)

        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')

# Instantiate the model
model = UpdatedNCFModel(num_users, num_items, num_genres)

# Train the model
train_model(model, train_loader, val_loader, epochs=5)

import torch
from torch import nn, optim
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# Assuming df is your DataFrame loaded with the dataset
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# Function to prepare DataLoader
def prepare_data_loader(df, user_encoder, movie_encoder, genre_columns, batch_size=32):
    user_ids = torch.tensor(user_encoder.transform(df['userId'].values), dtype=torch.long)
    movie_ids = torch.tensor(movie_encoder.transform(df['movieId'].values), dtype=torch.long)
    genres = torch.tensor(df[genre_columns].values, dtype=torch.float)
    ratings = torch.tensor(df['rating'].values, dtype=torch.float).unsqueeze(1)  # Make sure ratings are a column vector
    dataset = TensorDataset(user_ids, movie_ids, genres, ratings)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_loader = prepare_data_loader(train_df, user_encoder, movie_encoder, genre_columns)
val_loader = prepare_data_loader(val_df, user_encoder, movie_encoder, genre_columns)

# Model training and evaluation function
def train_and_evaluate_model(model, train_loader, val_loader, epochs=5, lr=0.001):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        train_losses = []
        for user_ids, movie_ids, genres, ratings in train_loader:
            optimizer.zero_grad()
            outputs = model(user_ids, movie_ids, genres)
            loss = criterion(outputs, ratings)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

        train_mse = np.mean(train_losses)

        model.eval()
        val_losses = []
        with torch.no_grad():
            for user_ids, movie_ids, genres, ratings in val_loader:
                outputs = model(user_ids, movie_ids, genres)
                loss = criterion(outputs, ratings)
                val_losses.append(loss.item())

        val_mse = np.mean(val_losses)

        print(f'Epoch {epoch+1}: Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}')

# Instantiate and train the model
model = UpdatedNCFModel(num_users=df['userId'].nunique(), num_items=df['movieId'].nunique(), num_genres=len(genre_columns))
train_and_evaluate_model(model, train_loader, val_loader, epochs=5)

################################

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model

# Load the dataset
df = pd.read_csv('df_dropped_columns.csv')

# Assume additional features exist, for demonstration purposes
# If additional features do not exist in your dataset, you'll need to adjust accordingly

# Ensure userId and movieId are treated as categorical features
user_encoder = LabelEncoder()
movie_encoder = LabelEncoder()

df['userId'] = user_encoder.fit_transform(df['userId'])
df['movieId'] = movie_encoder.fit_transform(df['movieId'])

# Split the data
train, test = train_test_split(df, test_size=0.2, random_state=42)

# Define the model creation function
def create_ncf_model(num_users, num_movies, embedding_size=50):
    # User input and embedding
    user_input = Input(shape=(1,))
    user_embedding = Embedding(num_users, embedding_size)(user_input)
    user_vec = Flatten()(user_embedding)

    # Movie input and embedding
    movie_input = Input(shape=(1,))
    movie_embedding = Embedding(num_movies, embedding_size)(movie_input)
    movie_vec = Flatten()(movie_embedding)

    # Concatenate user and movie vectors
    concat = Concatenate()([user_vec, movie_vec])

    # Dense layers
    dense = Dense(128, activation='relu')(concat)
    dense = Dense(64, activation='relu')(dense)
    output = Dense(1, activation='sigmoid')(dense)

    model = Model(inputs=[user_input, movie_input], outputs=output)
    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Assuming the max userId and movieId can be obtained from the encoders
num_users = df['userId'].nunique()
num_movies = df['movieId'].nunique()

ncf_model = create_ncf_model(num_users, num_movies)

# Prepare inputs
X_train = [train['userId'].values, train['movieId'].values]
y_train = (train['rating'] >= 4).astype(int).values  # Assuming 4+ as liked

# Train the model
history = ncf_model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.1)

# Save the model for deployment
ncf_model.save('/mnt/data/ncf_model.h5')

def recommend_movie(user_id, selected_movie_ids, model):
    """
    Recommend a movie from the selected_movie_ids based on the user's preferences.
    """
    # Transform user_id and movie_ids to match the trained model's encoding
    user_id_encoded = user_encoder.transform([user_id])
    movie_ids_encoded = movie_encoder.transform(selected_movie_ids)

    # Predict the scores for the selected movies
    predictions = model.predict([np.array([user_id_encoded] * len(selected_movie_ids)), movie_ids_encoded])

    # Find the index of the movie with the highest score
    recommended_index = np.argmax(predictions)

    # Decode the recommended movie ID
    recommended_movie_id = selected_movie_ids[recommended_index]

    return recommended_movie_id

# Example usage
user_id = 1  # Example user ID
selected_movie_ids = [24, 25, 26]  # Example movie IDs to choose from
recommended_movie_id = recommend_movie(user_id, selected_movie_ids, ncf_model)
print(f"Recommended Movie ID: {recommended_movie_id}")



#################################################################################################

# def find_similar_tags(row, genre_columns, df):
#     similar_films = df[(df[genre_columns] & row[genre_columns]).any(axis=1)]

#     similar_tags = similar_films[similar_films['tag'].notna()]['tag']

#     if not similar_tags.empty:
#         return similar_tags.sample(n=1).iloc[0]
#     else:
#         return 'No label'
# genre_columns  = genres_encoded_df.columns.tolist()
# print(genre_columns)
# merged_df['tag'] = merged_df.apply(lambda row: find_similar_tags(row, genre_columns, merged_df) if pd.isna(row['tag']) else row['tag'], axis=1)

import pandas as pd

# فرض می‌کنیم که ratings_df و tags_df از قبل وجود دارند و 'datetime' آن‌ها به فرمت datetime تبدیل شده‌است

# 1. گروه‌بندی و پیدا کردن اولین تاریخ برای هر دو دیتاست
earliest_ratings = ratings_df.groupby(['userId', 'movieId'])['datetime'].min().reset_index(name='earliest_rating_datetime')
earliest_tags = tags_df.groupby(['userId', 'movieId'])['datetime'].min().reset_index(name='earliest_tag_datetime')

# 2. ترکیب این دو دیتاست برای پیدا کردن اولین تاریخ کلی
combined_earliest = pd.merge(earliest_ratings, earliest_tags, on=['userId', 'movieId'])
combined_earliest
# تعیین اولین تاریخ کلی
# combined_earliest['earliest_datetime'] = combined_earliest.apply(
#     lambda x: min(x.dropna()[['earliest_rating_datetime', 'earliest_tag_datetime']]),
#     axis=1
# )
# 3. حفظ اطلاعات 'rating' و 'tag' برای اولین تاریخ
# مرج داده‌های اصلی با combined_earliest برای حفظ 'rating' و 'tag'
# merged_ratings = pd.merge(combined_earliest, ratings_df, on=['userId', 'movieId'], how='left')
# merged_tags = pd.merge(combined_earliest, tags_df, on=['userId', 'movieId'], how='left')

# # فیلتر کردن بر اساس اولین تاریخ
# merged_ratings = merged_ratings[merged_ratings['datetime'] == merged_ratings['earliest_datetime']]
# merged_tags = merged_tags[merged_tags['datetime'] == merged_tags['earliest_datetime']]

# # ترکیب نهایی اطلاعات 'rating' و 'tag' با استفاده از outer join برای اطمینان از حفظ تمامی اطلاعات
# final_df = pd.merge(merged_ratings[['userId', 'movieId', 'rating', 'earliest_datetime']], merged_tags[['userId', 'movieId', 'tag', 'earliest_datetime']], on=['userId', 'movieId', 'earliest_datetime'], how='outer')

# # print(final_df)
# cleaned_df = final_df.dropna(subset=['rating'], how='any')

# cleaned_df

from datetime import datetime


earliest_ratings = ratings_df.groupby(['userId', 'movieId'])['datetime'].min().reset_index(name='earliest_rating_datetime')

earliest_tags = tags_df.groupby(['userId', 'movieId'])['datetime'].min().reset_index(name='earliest_tag_datetime')

combined_earliest = pd.merge(earliest_ratings, earliest_tags, on=['userId', 'movieId'], how='outer')

combined_earliest['earliest_datetime'] = combined_earliest.apply(
    lambda row: row['earliest_rating_datetime'] if pd.isna(row['earliest_tag_datetime'])
    else (row['earliest_tag_datetime'] if pd.isna(row['earliest_rating_datetime'])
          else min(row['earliest_rating_datetime'], row['earliest_tag_datetime'])),
    axis=1
)
merged_ratings = pd.merge(combined_earliest, ratings_df, on=['userId', 'movieId'], how='left')
merged_tags = pd.merge(combined_earliest, tags_df, on=['userId', 'movieId'], how='left')

# فیلتر کردن بر اساس اولین تاریخ
merged_ratings = merged_ratings[merged_ratings['datetime'] == merged_ratings['earliest_datetime']]
merged_tags = merged_tags[merged_tags['datetime'] == merged_tags['earliest_datetime']]

# ترکیب نهایی اطلاعات 'rating' و 'tag' با استفاده از outer join برای اطمینان از حفظ تمامی اطلاعات
final_df = pd.merge(merged_ratings[['userId', 'movieId', 'rating', 'earliest_datetime']], merged_tags[['userId', 'movieId', 'tag', 'earliest_datetime']], on=['userId', 'movieId', 'earliest_datetime'], how='outer')

# print(final_df)
# has_NaT_values = final_df['tag'].isna().any()

# if has_NaT_values:
#     print("yes")
# else:
#     print("no")
# combined_earliest

import numpy as np

# Define a simple time decay function for ratings
# This is a placeholder function; in a real scenario, you might want to use a more sophisticated time decay formula
def apply_time_decay(days_since_earliest, max_days, half_life=365):
    """
    Apply a simple exponential decay based on the number of days since the earliest rating.
    - days_since_earliest: The number of days since the earliest rating.
    - max_days: The maximum number of days since the earliest rating in the dataset.
    - half_life: The half-life period in days. By default, set to 365 days.
    """
    # Normalize days to a 0-1 scale
    normalized_days = days_since_earliest / max_days
    # Apply exponential decay
    decay_factor = np.exp(-np.log(2) * normalized_days / (half_life / max_days))
    return decay_factor

# Apply the time decay to each rating
max_days = ratings_df['days_since_earliest'].max()
ratings_df['decay_factor'] = ratings_df['days_since_earliest'].apply(lambda x: apply_time_decay(x, max_days))

# Display the updated ratings dataframe with the decay factor
ratings_df[['userId', 'movieId', 'rating', 'days_since_earliest', 'decay_factor']].head()

# genres_encoded_df.to_csv('genres_encoded_df.csv')
# user_profiles.to_csv('user_profiles.csv')
# ratings_df.to_csv('ratings_df.csv')

# Since the user has already provided the preprocessed data in the form of variables, we will use these to merge and prepare a single dataset.
# The datasets are: tags_df (tags), genres_encoded_df (one-hot encoded genres), ratings_df (with decay factor), user_profiles, and links_df.

# First, we will merge the ratings dataframe with the movies dataframe to include movie titles and genres.
# Then, we will add the one-hot encoded genres to this merged dataframe.
# User profiles are aggregated data and might be used separately for user-specific recommendations rather than merged into the main dataset.
# The links dataframe can be merged based on movieId to add external IDs (IMDb, TMDb).

# Preparing the genres_encoded_df for merge
genres_encoded_df = genres_encoded_df.reset_index()  # Ensure movieId is a column for merging

# Merging movies with their ratings
movies_ratings_df = pd.merge(movies_df, ratings_df, on='movieId')

# Merging the above with genres_encoded_df
movies_ratings_genres_df = pd.merge(movies_ratings_df, genres_encoded_df, on='movieId', how='left')

# Merging the links dataframe to add IMDb and TMDb IDs
final_df = pd.merge(movies_ratings_genres_df, links_df, on='movieId', how='left')

# Display the first few rows of the final merged dataframe to check the structure
final_df.head()
# Assuming `final_dataset_df` is your main dataset and `user_profiles_df` contains the user profiles
# Merge user profile information into the main dataset
final_df = pd.merge(final_df, user_profiles, on='userId', how='left')

# Proceed with data splitting, model training, and evaluation as before, using the augmented dataset

# final_df.to_csv('final_df.csv')

# # To check for missing values and outliers in the final integrated dataset, we will perform the following steps:

# # 1. Check for missing values across all columns.
# missing_values = final_df.isnull().sum()

# # 2. Statistical summary to identify potential outliers, focusing on numerical columns like 'rating', 'timestamp', and any other relevant metrics.
# statistical_summary = final_df.describe()

# missing_values, statistical_summary

# Check for NaNs or infinite values in features and ratings
assert not torch.isnan(features).any(), "NaN values found in features"
assert not torch.isinf(features).any(), "Infinite values found in features"
assert not torch.isnan(ratings).any(), "NaN values found in ratings"
assert not torch.isinf(ratings).any(), "Infinite values found in ratings"

!pip install LibRecommender
import numpy as np
import pandas as pd
from libreco.data import random_split, DatasetPure
from libreco.algorithms import NCF  # pure data,
from libreco.evaluation import evaluate



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load the dataset
# final_df = pd.read_csv('/path/to/final_df.csv')

# Encode user and movie IDs
user_encoder = LabelEncoder()
item_encoder = LabelEncoder()

final_df['userId'] = user_encoder.fit_transform(final_df['userId'])
final_df['movieId'] = item_encoder.fit_transform(final_df['movieId'])

# Selecting features and target variable
X = final_df.drop(['rating', 'title', 'genres', 'genres_list', 'timestamp', 'datetime', 'imdbId', 'tmdbId'], axis=1)
y = final_df['rating']

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import torch
from torch.utils.data import Dataset, DataLoader

class MovieDataset(Dataset):
    def __init__(self, features, ratings):
        self.features = features
        self.ratings = ratings

    def __len__(self):
        return len(self.ratings)

    def __getitem__(self, idx):
        return {
            'features': torch.tensor(self.features.iloc[idx].values, dtype=torch.float),
            'rating': torch.tensor(self.ratings.iloc[idx], dtype=torch.float)
        }

train_dataset = MovieDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

import torch
import torch.nn as nn
import torch.nn.functional as F

class NCFModel(nn.Module):
    def __init__(self, num_features, embedding_dim=16):
        super(NCFModel, self).__init__()
        self.fc1 = nn.Linear(num_features, 128)
        self.fc2 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 1)

    def forward(self, features):
        x = F.relu(self.fc1(features))
        x = F.relu(self.fc2(x))
        x = self.output(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = NCFModel(num_features=X_train.shape[1]).to(device)

# Assuming criterion and optimizer are defined as before
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop adjusted for the updated model input structure
model.train()  # Set the model to training mode
for epoch in range(10):  # Number of epochs
    for batch in train_loader:
        # Retrieve the features and target ratings from the batch
        features = batch['features'].to(device)  # Move features to the correct device
        ratings = batch['rating'].to(device)     # Move ratings to the correct device

        optimizer.zero_grad()  # Clear gradients for this training step
        predictions = model(features).squeeze()  # Generate predictions
        loss = criterion(predictions, ratings)  # Compute the loss between predictions and actual ratings
        loss.backward()  # Perform backpropagation to calculate gradients
        optimizer.step()  # Update model parameters

        print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Assuming X_test and y_test are already defined from the train_test_split operation

test_dataset = MovieDataset(X_test, y_test)  # Initialize your dataset with test data
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # Create DataLoader for test data
model.eval()  # Set the model to evaluation mode
total_loss = 0
total_count = 0

with torch.no_grad():  # No need to track gradients during evaluation
    for batch in test_loader:
        features = batch['features'].to(device)  # Load features to the correct device
        ratings = batch['rating'].to(device)     # Load ratings to the correct device

        predictions = model(features).squeeze()  # Generate predictions
        loss = criterion(predictions, ratings)  # Calculate loss

        total_loss += loss.item() * features.size(0)  # Update total loss
        total_count += features.size(0)  # Update count
        predictions = model(features).squeeze()
        if torch.isnan(predictions).any():
            print("NaN values detected in predictions")

# Calculate average loss across all test data
average_loss = total_loss / total_count
print(f"Average Test Loss: {average_loss}")

import torch
import numpy as np
def recommend_movie(model, user_id, candidate_movies, device='cpu'):
    model.eval()  # Set the model to evaluation mode
    recommendations = []

    # Example feature construction adjusted for demonstration purposes
    for movie_id in candidate_movies:
        # Construct features tensor with correct size
        # This should include user features, movie features, and any other required features
        # Adjust this part to match your actual data structure
        features = np.zeros((1, 44))  # Placeholder: Construct a features array with the correct size

        # Example: Assuming the first two features are user_id and movie_id
        features[0, 0] = user_id  # Adjust as per your feature encoding
        features[0, 1] = movie_id  # Adjust as per your feature encoding

        # Convert features to a tensor and move to the correct device
        features_tensor = torch.tensor(features, dtype=torch.float).to(device)

        # Predict the score for the user and this movie
        score = model(features_tensor).item()  # Get the score as a scalar
        recommendations.append((movie_id, score))

    # Sort the recommendations based on scores in descending order
    recommendations.sort(key=lambda x: x[1], reverse=True)

    # Return the movie ID with the highest score
    top_recommendation = recommendations[0][0]
    return top_recommendation

# فرض کنید user_id معرف یک کاربر خاص و candidate_movies شامل لیستی از فیلم‌ها است
user_id = 123  # مثال: شناسه کاربر
candidate_movies = [10, 20, 30, 40]  # مثال: شناسه‌های فیلم‌های کاندید

# اجرای تابع پیشنهاددهنده
top_movie_id = recommend_movie(model, user_id, candidate_movies, device)

print(f"Top movie ID recommended: {top_movie_id}")

def recommend_for_user(model, user_id):
    model.eval()  # Set the model to evaluation mode

    # تبدیل شناسه کاربر به تنسور و اضافه کردن بعد برای batch (اگر لازم است)
    # تغییر dtype به Float
    user_tensor = torch.tensor([user_id], dtype=torch.float).unsqueeze(0)

    # اجرای پیش‌بینی مدل
    with torch.no_grad():  # فعال نکردن محاسبه گرادیان
        prediction = model(user_tensor)
        # اینجا فرض می‌کنیم مدل یک شناسه فیلم به عنوان خروجی برمی‌گرداند
        recommended_movie_id = prediction.argmax().item()  # انتخاب فیلم با بالاترین امتیاز

    return recommended_movie_id

user_id = 123  # فرضی

# فراخوانی تابع پیشنهاددهنده
recommended_movie_id = recommend_for_user(model, user_id)

print(f"Recommended Movie ID for User {user_id}: {recommended_movie_id}")

# #1
# torch.save(model, 'model_complete.pth')
# #2
# torch.save(model.state_dict(), 'model_state_dict.pth')
# model = NCFModel(...)  # ابتدا مدل را با استفاده از معماری اصلی تعریف کنید
# model.load_state_dict(torch.load('model_state_dict.pth'))
# model.eval()  # مدل را در حالت ارزیابی قرار دهید
# #3
# model = torch.load('model_complete.pth')





train_data, data_info = DatasetPure.build_trainset(train_data)
eval_data = DatasetPure.build_evalset(eval_data)
test_data = DatasetPure.build_testset(test_data)

ncf = NCF(
    task="rating",
    data_info=data_info,
    loss_type="cross_entropy",
    embed_size=16,
    n_epochs=10,
    lr=1e-3,
    batch_size=2048,
    num_neg=1,
)
# monitor metrics on eval data during training
ncf.fit(
    train_data,
    neg_sampling=False, #for rating, this param is false else True
    verbose=2,
    eval_data=eval_data,
    metrics=["loss"],
)

# do final evaluation on test data
evaluate(
    model=ncf,
    data=test_data,
    neg_sampling=False,
    metrics=["loss"],
)
# # Predict preference for a specific user and movie
# predicted_rating = ncf.predict(user=userId, item=movieId)

# # Recommend a list of movies for a specific user
# recommendations = ncf.recommend_user(user=userId, n_rec=10)
# predict preference of user 5755 to item 110
ncf.predict(userId=5755, movieId=110)

# recommend 10items for user 5755
ncf.recommend_user(userId=5755, n_rec=10)









# !pip install LibRecommender
# import numpy as np
# import pandas as pd
# from libreco.data import random_split, DatasetPure
# from libreco.algorithms import NCF  # pure data,
# from libreco.evaluation import evaluate

# train_data, eval_data, test_data = random_split(data, multi_ratios=[0.8, 0.1, 0.1])
# train_data, data_info= DatasetPure.build_trainset(train_data)
# eval_data = DatasetPure.build_evalset(eval_data)
# test_data = DatasetPure.build_testset(test_data)
# ncf = NCF(
#     task="rating",
#     data_info=data_info,
#     loss_type="cross_entropy",
#     embed_size=16,
#     n_epochs=10,
#     lr=1e-3,
#     batch_size=2048,
#     num_neg=1,
# )
# # monitor metrics on eval data during training
# ncf.fit(
#     train_data,
#     neg_sampling=False, #for rating, this param is false else True
#     verbose=2,
#     eval_data=eval_data,
#     metrics=["loss"],
# )

# # do final evaluation on test data
# evaluate(
#     model=ncf,
#     data=test_data,
#     neg_sampling=False,
#     metrics=["loss"],
# )
# #for implicit feedback, metrics like precision@k, recall@k, ndcg can be used
# # predict preference of user 5755 to item 110
# ncf.predict(user=5755, item=110)

# # recommend 10items for user 5755
# ncf.recommend_user(user=5755, n_rec=10)

import pandas as pd

# Load your dataset


# Assuming the dataset is already split and in the correct format
train_data = ratings_df.sample(frac=0.8, random_state=42)  # Randomly sample 80% of the data for training
test_data = ratings_df.drop(train_data.index)  # Use the rest for testing
test_data.head()

import torch
import torch.nn as nn
import torch.nn.functional as F

class NCFModel(nn.Module):
    def __init__(self, num_users, num_items, num_genres, embedding_dim=32):
        super(NCFModel, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2 + num_genres + 1, 128)  # Additional inputs for genres and decay factor
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.dropout = nn.Dropout(0.2)

    def forward(self, user_ids, item_ids, genres, decay_factors):
        user_embedded = self.user_embedding(user_ids)
        item_embedded = self.item_embedding(item_ids)
        x = torch.cat([user_embedded, item_embedded, genres, decay_factors.unsqueeze(1)], dim=1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        return x

from torch.utils.data import Dataset, DataLoader
import numpy as np
genres_encoded_df = genres_encoded_df.set_index(movies_df['movieId'])

# class MovieDataset(Dataset):
#     def __init__(self, ratings, user_profiles, genres_encoded_df, decay_factors):
#         self.users = ratings['userId'].values
#         self.items = ratings['movieId'].values
#         self.ratings = ratings['rating'].values
#         self.genres = genres_encoded_df.loc[self.items].values  # Match movies with their genres
#         self.decay_factors = decay_factors.values  # Assuming decay_factors is a Series aligned with ratings

#     def __len__(self):
#         return len(self.ratings)

#     def __getitem__(self, idx):
#         return (
#             torch.tensor(self.users[idx], dtype=torch.long),
#             torch.tensor(self.items[idx], dtype=torch.long),
#             torch.tensor(self.genres[idx], dtype=torch.float),
#             torch.tensor(self.decay_factor[idx], dtype=torch.float),
#             torch.tensor(self.ratings[idx], dtype=torch.float)
#         )
class MovieDataset(Dataset):
    def __init__(self, ratings, user_profiles, genres_encoded_df, decay_factors):
        self.users = ratings['userId'].values
        self.items = ratings['movieId'].values
        self.ratings = ratings['rating'].values
        self.genres = genres_encoded_df.loc[self.items].values  # Assuming genres_encoded_df is correctly indexed by movieId
        self.decay_factors = decay_factors.values  # Ensure this is correctly aligned with your ratings DataFrame

    def __len__(self):
        return len(self.ratings)

    def __getitem__(self, idx):
        return (
            torch.tensor(self.users[idx], dtype=torch.long),
            torch.tensor(self.items[idx], dtype=torch.long),
            torch.tensor(self.genres[idx], dtype=torch.float),
            torch.tensor(self.decay_factors[idx], dtype=torch.float),  # Corrected attribute name here
            torch.tensor(self.ratings[idx], dtype=torch.float)
        )


# Assuming `ratings_df`, `user_profiles`, and `genres_encoded_df` are already defined
decay_factors = ratings_df['decay_factor']  # Make sure this aligns with your ratings DataFrame
dataset = MovieDataset(ratings_df, user_profiles, genres_encoded_df, decay_factors)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Ensure that IDs start from 0
ratings_df['userId'] = ratings_df['userId'] - ratings_df['userId'].min()
ratings_df['movieId'] = ratings_df['movieId'] - ratings_df['movieId'].min()

# Recalculate the number of unique users and movies
num_users = ratings_df['userId'].nunique()
num_movies = ratings_df['movieId'].max() + 1  # Assuming movieId is contiguous

# Update the model instantiation with these new counts
model = NCFModel(num_users=num_users, num_items=num_movies, num_genres=len(mlb.classes_), embedding_dim=32)

# Ensure all IDs are remapped in your dataset similarly

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = NCFModel(num_users=ratings_df['userId'].nunique(), num_items=movies_df['movieId'].nunique(), num_genres=len(mlb.classes_), embedding_dim=32)
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for user_ids, item_ids, genres, decay_factors, ratings in loader:
        user_ids, item_ids, genres, decay_factors, ratings = user_ids.to(device), item_ids.to(device), genres.to(device), decay_factors.to(device), ratings.to(device)
        optimizer.zero_grad()
        outputs = model(user_ids, item_ids, genres, decay_factors).squeeze()
        loss = criterion(outputs, ratings)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")





import torch
from torch import nn

class NCF(nn.Module):
    def __init__(self, num_users, num_items, num_genres, embedding_dim, genre_feature_dim):
        super(NCF, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        self.genre_embedding = nn.Linear(num_genres, genre_feature_dim) if num_genres else None
        self.fc_layers = nn.Sequential(
            nn.Linear(embedding_dim * 2 + genre_feature_dim + 1, 128),  # +1 for the timestamp feature
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # Predicting a rating
        )

    def forward(self, user_ids, item_ids, genres, timestamps):
        user_embedded = self.user_embedding(user_ids)
        item_embedded = self.item_embedding(item_ids)
        genre_features = self.genre_embedding(genres) if self.genre_embedding else genres
        x = torch.cat([user_embedded, item_embedded, genre_features, timestamps.unsqueeze(-1)], dim=1)
        return self.fc_layers(x)

import torch
from torch import nn

class NCFModel(nn.Module):
    def __init__(self, num_users, num_items, num_genres, embedding_dim=32):
        super(NCFModel, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        self.genre_fc = nn.Linear(num_genres, embedding_dim)  # Assuming genres are one-hot encoded
        self.fc_layers = nn.Sequential(
            nn.Linear(embedding_dim * 3, 128),  # for user, item, genre embeddings
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # Predicting a single rating value
        )

    def forward(self, user_ids, item_ids, genres):
        user_embedded = self.user_embedding(user_ids)
        item_embedded = self.item_embedding(item_ids)
        genre_embedded = self.genre_fc(genres.float())  # Ensure genres is a float tensor
        combined_features = torch.cat([user_embedded, item_embedded, genre_embedded], dim=-1)
        return self.fc_layers(combined_features)

from torch.utils.data import Dataset, DataLoader

class MovieDataset(Dataset):
    def __init__(self, users, items, genres, ratings):
        self.users = users
        self.items = items
        self.genres = genres
        self.ratings = ratings

    def __len__(self):
        return len(self.ratings)

    def __getitem__(self, idx):
        return {
            "user": self.users[idx],
            "item": self.items[idx],
            "genre": self.genres[idx],
            "rating": self.ratings[idx]
        }

# Assuming users, items, genres, and ratings are all numpy arrays or similar
dataset = MovieDataset(users, items, genres, ratings)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

from torch.utils.data import Dataset, DataLoader

class MovieDataset(Dataset):
    def __init__(self, users, items, genres, ratings):
        self.users = users
        self.items = items
        self.genres = genres
        self.ratings = ratings

    def __len__(self):
        return len(self.ratings)

    def __getitem__(self, idx):
        return {
            "user": self.users[idx],
            "item": self.items[idx],
            "genre": self.genres[idx],
            "rating": self.ratings[idx]
        }

# Assuming users, items, genres, and ratings are all numpy arrays or similar
dataset = MovieDataset(users, items, genres, ratings)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

import torch
from torch import nn

class ExpandedNCFModel(nn.Module):
    def __init__(self, num_users, num_items, num_genres, num_timestamp_bins, embedding_dim=32):
        super(ExpandedNCFModel, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        self.genre_linear = nn.Linear(num_genres, embedding_dim)  # Assuming one-hot encoded genres
        self.timestamp_embedding = nn.Embedding(num_timestamp_bins, embedding_dim)  # For timestamp bins
        self.fc_layers = nn.Sequential(
            nn.Linear(embedding_dim * 4, 128),  # Adjusted for additional timestamp input
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # Predicting a single rating value
        )

    def forward(self, user_ids, item_ids, genres, timestamps):
        user_embedded = self.user_embedding(user_ids)
        item_embedded = self.item_embedding(item_ids)
        genre_embedded = self.genre_linear(genres.float())
        timestamp_embedded = self.timestamp_embedding(timestamps)
        combined_features = torch.cat([user_embedded, item_embedded, genre_embedded, timestamp_embedded], dim=-1)
        return self.fc_layers(combined_features)

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset, random_split

# Assuming you have a dataset class similar to the previous example
dataset = MovieDataset(users, items, genres, ratings, timestamps)  # Adjusted to include timestamps

# Split the dataset into training and validation sets
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Model instantiation, loss function, and optimizer as before
model = ExpandedNCFModel(num_users, num_items, num_genres, num_timestamp_bins).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop including validation
for epoch in range(num_epochs):
    model.train()
    for batch in train_loader:
        # Training steps as before, including zero_grad, loss calculation, backward, and step
        pass  # Replace with actual training code

    model.eval()
    with torch.no_grad():
        # Loop over validation loader to calculate validation loss
        pass  # Replace with validation code

    print(f"Epoch {epoch+1}, Training Loss: {...}, Validation Loss: {...}")

from torch.nn.utils import prune
import torch.quantization

# Assuming `model` is your trained PyTorch model
# Pruning 20% of connections in the first linear layer of your model
prune.l1_unstructured(model.fc_layers[0], name='weight', amount=0.2)

# Quantization
model.eval()  # Ensure the model is in evaluation mode before quantization
quantized_model = torch.quantization.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)

from torch.utils.data import DataLoader

# Example DataLoader setup
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)

def recommend_user(model, user_id, n_rec=10):
    # Assume we have a list or array of all item IDs
    all_items = np.arange(num_items)  # This would be replaced by your actual item IDs

    # Convert user_id and all item_ids to tensors for model input
    user_tensor = torch.tensor([user_id] * len(all_items))  # Repeat user_id for each item
    items_tensor = torch.tensor(all_items)

    # Ensure model is in evaluation mode
    model.eval()

    with torch.no_grad():
        # Predict the user's preference for all items
        predictions = model(user_tensor, items_tensor).flatten()

    # Get the top-n recommendations; argsort returns indices of sorted values
    recommended_item_indices = predictions.argsort(descending=True)[:n_rec]
    recommended_items = all_items[recommended_item_indices]

    return recommended_items

cache = {}  # Initialize a cache

def get_recommendations(user_id):
    if user_id in cache:
        return cache[user_id]  # Return cached recommendations if available
    else:
        # Assuming a function `recommend_user` to generate recommendations
        recommendations = recommend_user(user_id, n_rec=10)
        cache[user_id] = recommendations  # Store recommendations in cache
        return recommendations

# Pseudocode for caching
cache = {}

def get_recommendations(user_id):
    if user_id in cache:
        return cache[user_id]
    else:
        recommendations = model.predict(user_id)  # Simplified prediction call
        cache[user_id] = recommendations
        return recommendations